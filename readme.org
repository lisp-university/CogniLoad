* CogniLoad

** transformer
  逐步实现Lisp transformer的编码器和解码器。
  最终目标实现让模型生成式自举Lisp实现。
  
** RWKV (RNN)
  采用了RWKV训练模型，效率更高。
  项目地址：https://github.com/BlinkDL/RWKV-LM

** other
*** 20230502
  手写了部分CL版本的transformer模型代码，有些遭不住，毕竟不是人工智能相关专业的学生，今年恶补了些关于神经网络的基础知识，囫囵吞枣罢了，完全掌握尚需时日。
  另外CL代码写得越长我就越会忘记最前面的代码，可能需要拆分函数实现，干这件事于我而言很有挑战性，仍旧需要多多学习。
  不论是CNN、RNN还是其他什么网络，加入transformer这种带有位置编码的结构都能得到不错的效果，大量的数据集，逼近阈值以后就是另一个世界了。
  预训练Lisp模型的好处就是一旦模型掌握了S表达式的数据和代码的规律，就可以将很多数据完全转换成更清晰无误的表达，把REPL模式与模型的交互当作一种微调手段更快的获取结果。
  用SBCL保存的镜像状态可以更快速更容易的在其他环境中恢复，尽量减少不必要的依赖。相对而言Python的环境依赖就显得经不住折腾，但Python的资源确实丰富，torch非常棒。
  现在以及未来唯一能想到的快速训练大模型的方式是通过与其他现有大模型的无限生成交互获得，和其他大模型交互得越多的预训练模型能力会越接近，当然这是取巧的方式。
  
  大模型很适合做成操作系统的内核，模型驱动的系统好过用巨量代码累积起来的大山，在可遇见的未来切换大模型就可以获得不同能力的操作系统，还有什么比这更酷的吗！
  大模型的多模态也很有前景，一个6gb的模型胜过一个6tb的数据库，这些都是很容易看清楚的未来。
  
  这次训练CogniLoad-ModeLisp使用了三个主要实现的代码作为训练数据集：Common Lisp、Scheme和Emacs Lisp，其他方言的实现和类Lisp的语言并不在内，或许训练一个DSL模型也很有趣。
  Lisp-1和Lisp-2通用性更强，Elisp体量也很大，至少在多个平台上（包括手机）它们的一致性表现的很好。
  
** log
  训练模型CogniLoad-ModeLisp.pth (python) 20230501
  创建项目时间戳20230405
