* CogniLoad

** transformer
  逐步实现Lisp transformer的编码器和解码器。
  最终目标实现让模型生成式自举Lisp实现。
  
** RWKV (RNN)
  采用了RWKV训练模型，效率更高。
  项目地址：https://github.com/BlinkDL/RWKV-LM

** other
*** 20230502
  手写了部分CL版本的transformer模型代码，有些遭不住，毕竟不是人工智能相关专业的学生，今年恶补了些关于神经网络的基础知识，囫囵吞枣罢了，完全掌握尚需时日。
  另外CL代码写得越长我就越会忘记最前面的代码，可能需要拆分函数实现，干这件事于我而言很有挑战性，仍旧需要多多学习。
  不论是CNN、RNN还是其他什么网络，加入transformer这种带有位置编码的结构都能得到不错的效果，大量的数据集，逼近阈值以后就是另一个世界了。
  预训练Lisp模型的好处就是一旦模型掌握了S表达式的数据和代码的规律，就可以将很多数据完全转换成更清晰无误的表达，把REPL模式与模型的交互当作一种微调手段更快的获取结果。
  用SBCL保存的镜像状态可以更快速更容易的在其他环境中恢复，尽量减少不必要的依赖。相对而言Python的环境依赖就显得经不住折腾，但Python的资源确实丰富，torch非常棒。
  现在以及未来唯一能想到的快速训练大模型的方式是通过与其他现有大模型的无限生成交互获得，和其他大模型交互得越多的预训练模型能力会越接近，当然这是取巧的方式。
  
** log
  训练模型CogniLoad-ModeLisp.pth (python) 20230501
  创建项目时间戳20230405
